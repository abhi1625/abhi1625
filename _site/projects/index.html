<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>
         Abhinav Modi &middot; Projects 
    </title>
    <meta name="description" content="Abhinav Modi is a graduate student pursuing his Masters in Robotics at the University of Maryland. His research interests include mobile robotics, state estimation, computer vision and machine learning.">

    <link href="https://fonts.googleapis.com/css?family=Lato:300,700" rel="stylesheet">

    <link href="/assets/css/main.css" rel="stylesheet">
    <link href="http://localhost:4000/projects/" rel="canonical">

    <link href="/assets/css/fontawesome-all.min.css" rel="stylesheet">

    <link href="/assets/favicon.png" rel="apple-touch-icon">
</head>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <nav class="site-nav">

      <div class="trigger">
        
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
            
              <a class="page-link" href="/">Home</a>
            
          
        
          
            
              <a class="page-link" href="/projects/">Projects</a>
            
          
        
        <a class="page-link" href="/assets/docs/cv.pdf" target="_blank">CV</a>
      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Projects</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content">
    <p>The following is a complete list of my scholarly publications, which touch on mobile robotics, state estimation, computer vision and machine learning, among other things.</p>

<p>For more information, visit my <a href="//scholar.google.com/citations?user=skVilbAAAAAJ" target="_blank"><b>Google Scholar page</b> <i class="fa fa-external-link"></i></a>.</p>

<hr />

<div>




    <div>
    <br />

    <h3>Visual Appearance Estimation</h3><br />

    
        <img class="right" style="width: 40%; padding-left: 1em" src="/assets/img/appearance_change.png" />
    

    

    

        
            <a href="//doi.org/10.1109/LRA.2018.2799741" target="_blank"><b>How to Train a CAT: Learning Canonical Appearance Transformations for Direct Visual Localization Under Illumination Change</b></a>
        
        <br />

        <i>Lee Clement and Jonathan Kelly</i>
        <br />

        IEEE Robotics and Automation Letters (RA-L), 2018
        <br />

        
            <i>Presented at the IEEE International Conference on Robotics and Automation (ICRA), Brisbane, Australia, 21 - 25 May 2018</i><br />
        

        

        
            <a href="//arxiv.org/pdf/1709.03009" target="_blank"><i class="far fa-file-alt"></i>&nbsp;Preprint</a>&nbsp;
        

        
            <a href="//github.com/utiasSTARS/cat-net" target="_blank"><i class="fas fa-code-branch"></i>&nbsp;Code</a>&nbsp;
        

        

        
            <a href="/assets/docs/icra2018_cat_poster.pdf" target="_blank"><i class="far fa-image"></i>&nbsp;Poster</a>&nbsp;
        

        
            <a href="//youtu.be/ej6VNBq3dDE" target="_blank"><i class="fab fa-youtube"></i>&nbsp;Video</a>&nbsp;
        

        
            <br />
        

        <br />
    

    </div>
    <hr />


    <div>
    <br />

    <h3>Visual Sun Detection for Accurate Visual Odometry</h3><br />

    
        <img class="right" style="width: 40%; padding-left: 1em" src="/assets/img/sun_estimation.png" />
    

    

    

        
            <a href="//doi.org/10.1177/0278364917749732" target="_blank"><b>Inferring Sun Direction to Improve Visual Odometry: A Deep Learning Approach</b></a>
        
        <br />

        <i>Valentin Peretroukhin*, Lee Clement* and Jonathan Kelly</i>
        <br />

        International Journal of Robotics Research (IJRR), Special Issue on Experimental Robotics, 2018
        <br />

        
            <i>*Equal contribution</i><br />
        

        

        

        
            <a href="//github.com/utiasSTARS/sun-bcnn" target="_blank"><i class="fas fa-code-branch"></i>&nbsp;Code</a>&nbsp;
        

        

        

        

        
            <br />
        

        <br />
    

        
            <a href="//dx.doi.org/10.1109/ICRA.2017.7989235" target="_blank"><b>Reducing Drift in Visual Odometry by Inferring Sun Direction using a Bayesian Convolutional Neural Network</b></a>
        
        <br />

        <i>Valentin Peretroukhin*, Lee Clement* and Jonathan Kelly</i>
        <br />

        In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), Singapore, 29 May - 3 June 2017
        <br />

        
            <i>*Equal contribution</i><br />
        

        

        
            <a href="//arxiv.org/pdf/1609.05993" target="_blank"><i class="far fa-file-alt"></i>&nbsp;Preprint</a>&nbsp;
        

        
            <a href="//github.com/utiasSTARS/sun-bcnn" target="_blank"><i class="fas fa-code-branch"></i>&nbsp;Code</a>&nbsp;
        

        
            <a href="/assets/docs/icra2017_sunbcnn_slides.pdf" target="_blank"><i class="fas fa-desktop"></i>&nbsp;Slides</a>&nbsp;
        

        
            <a href="/assets/docs/icra2017_sunbcnn_poster.pdf" target="_blank"><i class="far fa-image"></i>&nbsp;Poster</a>&nbsp;
        

        
            <a href="//youtu.be/c5XTrq3a2tE" target="_blank"><i class="fab fa-youtube"></i>&nbsp;Video</a>&nbsp;
        

        
            <br />
        

        <br />
    

        
            <a href="//dx.doi.org/10.1007/978-3-319-50115-4_36" target="_blank"><b>Improving the Accuracy of Stereo Visual Odometry Using Visual Illumination Estimation</b></a>
        
        <br />

        <i>Lee Clement, Valentin Peretroukhin, and Jonathan Kelly</i>
        <br />

        In Proceedings of the 2016 International Symposium on Experimental Robotics (ISER), Tokyo, Japan, 3 - 6 October 2016
        <br />

        

        
            <b>Toyota Student Participation Award; Invited to IJRR Special Issue</b><br />
        

        
            <a href="//arxiv.org/pdf/1609.04705" target="_blank"><i class="far fa-file-alt"></i>&nbsp;Preprint</a>&nbsp;
        

        

        
            <a href="/assets/docs/iser2016_sunVO_slides.pdf" target="_blank"><i class="fas fa-desktop"></i>&nbsp;Slides</a>&nbsp;
        

        

        

        
            <br />
        

        <br />
    

    </div>
    <hr />


    <div>
    <br />

    <h3>Monocular Visual Teach &amp; Repeat</h3><br />

    
        <img class="right" style="width: 40%; padding-left: 1em" src="/assets/img/mono_vtr.png" />
    

    

    

        
            <a href="//dx.doi.org/10.1002/rob.21655" target="_blank"><b>Robust Monocular Visual Teach and Repeat Aided by Local Ground Planarity and Colour-Constant Imagery</b></a>
        
        <br />

        <i>Lee Clement, Jonathan Kelly and Timothy D Barfoot</i>
        <br />

        Journal of Field Robotics, 34(1): 74â€“97, 2017
        <br />

        

        

        
            <a href="//www.starslab.ca/wp-content/papercite-data/pdf/2016_clement_robust.pdf" target="_blank"><i class="far fa-file-alt"></i>&nbsp;Preprint</a>&nbsp;
        

        

        

        

        

        
            <br />
        

        <br />
    

        
            <a href="//dx.doi.org/10.1007/978-3-319-27702-8_36" target="_blank"><b>Monocular Visual Teach and Repeat Aided by Local Ground Planarity</b></a>
        
        <br />

        <i>Lee Clement, Jonathan Kelly and Timothy D Barfoot</i>
        <br />

         In Proceedings of the 10th Conference on Field and Service Robotics (FSR), Toronto, Ontario, 24 - 26 June 2015
        <br />

        

        

        
            <a href="//arxiv.org/pdf/1707.08989" target="_blank"><i class="far fa-file-alt"></i>&nbsp;Preprint</a>&nbsp;
        

        

        
            <a href="/assets/docs/fsr2015_monoVTR_slides.pdf" target="_blank"><i class="fas fa-desktop"></i>&nbsp;Slides</a>&nbsp;
        

        

        
            <a href="//youtu.be/FU6KeWgwrZ4" target="_blank"><i class="fab fa-youtube"></i>&nbsp;Video</a>&nbsp;
        

        
            <br />
        

        <br />
    

        
            <b>Monocular Vision for Long-range Visual Teach and Repeat in Unstructured Environments</b>
        
        <br />

        <i>Lee Clement, Jonathan Kelly and Timothy D Barfoot</i>
        <br />

        Presented at the NSERC Canadian Field Robotics Network (NCFRN) and Conference on Computer and Robot Vision (CRV) Joint Poster Session Montreal, Quebec, 10 May 2014.
        <br />

        

        

        

        

        

        
            <a href="/assets/docs/ncfrn2014_monoVTR_poster.pdf" target="_blank"><i class="far fa-image"></i>&nbsp;Poster</a>&nbsp;
        

        

        
            <br />
        

        <br />
    

    </div>
    <hr />


    <div>
    <br />

    <h3>Miscellaneous</h3><br />

    
        <img class="right" style="width: 40%; padding-left: 1em" src="/assets/img/misc_sensors.png" />
    

    

    

        
            <a href="//dx.doi.org/10.1109/MFI.2016.7849530" target="_blank"><b>Entropy-based Sim(3) Calibration of 2D Lidars to Egomotion Sensors</b></a>
        
        <br />

        <i>Jacob Lambert, Lee Clement, Matthew Giamou and Jonathan Kelly</i>
        <br />

        In Proceedings of the 2016 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI), Baden Baden, Germany, 19 - 21 September 2016
        <br />

        

        
            <b>Best Student Paper Award</b><br />
        

        
            <a href="//arxiv.org/pdf/1707.08680" target="_blank"><i class="far fa-file-alt"></i>&nbsp;Preprint</a>&nbsp;
        

        

        
            <a href="/assets/docs/mfi2016_entropy_slides.pdf" target="_blank"><i class="fas fa-desktop"></i>&nbsp;Slides</a>&nbsp;
        

        

        

        
            <br />
        

        <br />
    

        
            <a href="//dx.doi.org/10.1109/IROS.2015.7353890" target="_blank"><b>PROBE: Predictive Robust Estimation for Visual-Inertial Navigation</b></a>
        
        <br />

        <i>Valentin Peretroukhin, Lee Clement, Matthew Giamou and Jonathan Kelly</i>
        <br />

        In Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Hamburg, Germany, 28 September - 2 October 2015
        <br />

        

        

        
            <a href="//www.starslab.ca/wp-content/papercite-data/pdf/2015_peretroukhin_probe.pdf" target="_blank"><i class="far fa-file-alt"></i>&nbsp;Preprint</a>&nbsp;
        

        

        
            <a href="/assets/docs/iros2015_PROBE_slides.pdf" target="_blank"><i class="fas fa-desktop"></i>&nbsp;Slides</a>&nbsp;
        

        

        
            <a href="//youtu.be/0YmdVJ0Be3Q" target="_blank"><i class="fab fa-youtube"></i>&nbsp;Video</a>&nbsp;
        

        
            <br />
        

        <br />
    

        
            <a href="//dx.doi.org/10.1109/CRV.2015.11" target="_blank"><b>The Battle for Filter Supremacy: A Comparative Study of the Multi-State Constraint Kalman Filter and the Sliding Window Filter</b></a>
        
        <br />

        <i>Lee Clement*, Valentin Peretroukhin*, Jacob Lambert and Jonathan Kelly</i>
        <br />

        In Proceedings of the 12th Conference on Computer and Robot Vision (CRV), Halifax, Nova Scotia, 3 - 5 June 2015
        <br />

        
            <i>*Equal contribution</i><br />
        

        

        
            <a href="//www.starslab.ca/wp-content/papercite-data/pdf/2015_clement_battle.pdf" target="_blank"><i class="far fa-file-alt"></i>&nbsp;Preprint</a>&nbsp;
        

        
            <a href="//github.com/utiasSTARS/msckf-swf-comparison" target="_blank"><i class="fas fa-code-branch"></i>&nbsp;Code</a>&nbsp;
        

        
            <a href="/assets/docs/crv2015_battle_slides.pdf" target="_blank"><i class="fas fa-desktop"></i>&nbsp;Slides</a>&nbsp;
        

        

        

        
            <br />
        

        <br />
    

        
            <b>Get to the Point: Active Covariance Scaling for Feature Tracking Through Motion Blur</b>
        
        <br />

        <i>Valentin Peretroukhin, Lee Clement and Jonathan Kelly</i>
        <br />

        Presented at the Workshop on Scaling Up Active Perception, IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, 26 - 30 May 2015
        <br />

        

        

        
            <a href="//www.starslab.ca/wp-content/papercite-data/pdf/2015_peretroukhin_get.pdf" target="_blank"><i class="far fa-file-alt"></i>&nbsp;Preprint</a>&nbsp;
        

        

        
            <a href="/assets/docs/icra2015_blur_slides.pdf" target="_blank"><i class="fas fa-desktop"></i>&nbsp;Slides</a>&nbsp;
        

        
            <a href="/assets/docs/icra2015_blur_poster.pdf" target="_blank"><i class="far fa-image"></i>&nbsp;Poster</a>&nbsp;
        

        

        
            <br />
        

        <br />
    

        
            <a href="//dx.doi.org/10.1109/CCECE.2013.6567796" target="_blank"><b>Implementation of a Nanosatellite Attitude Determination and Control System for the T-Sat1 Mission</b></a>
        
        <br />

        <i>Brady Russell, Lee Clement, Joshua Hernandez, Ahmad Byagowi, Dario Schor and Witold Kinsner</i>
        <br />

        In Proceedings of the Canadian Conference on Electrical and Computer Engineering (CCECE). Regina, Saskatchewan, 5 - 8 May 2013
        <br />

        

        

        

        

        

        

        

        
            <br />
        

        <br />
    

    </div>
    <hr />

</div>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer center">

  <div class="wrapper">
  	<p>This site was built using <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and <a href="http://liabogoev.com/-folio/" target="_blank">*folio</a> and is hosted on <a href=https://github.com/abhi1625/abhinavmodi.github.io target="_blank">Github</a> under the MIT License. &copy; 2019</p>
  </div>

</footer>


  </body>

</html>
